{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerimeterMaison/Assignment-4/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sTsDfIVKsmL"
      },
      "source": [
        "\n",
        "OBS: CTRL+F ASDF. ALLT SOM HAR ASDF EFTER SIG MÅSTE KORRIGERAS!\n",
        "\n",
        "/MoMoney\n",
        "\n",
        "# DAT565 Introduction to Data Science and AI\n",
        "## 2023-2024, LP2\n",
        "## Assignment 4: Spam classification using Naïve Bayes\n",
        "This assignment has three obligatory questions. Questions 4-5 are optional and will not be graded.\n",
        "\n",
        "\n",
        "The exercise takes place in this notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical.\n",
        "\n",
        "*Tips:*\n",
        "* You can execute certain Linux shell commands by prefixing the command with a `!`.\n",
        "* You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results, the second you can use to write code snippets that execute the tasks required.  \n",
        "\n",
        "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets.\n",
        "\n",
        "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location:\n",
        "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages.\n",
        "-\thard-ham: non-spam messages more difficult to differentiate\n",
        "-\tspam: spam messages\n",
        "\n",
        "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.**\n",
        "\n",
        "If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer. Note that if you are using Windows, you can instead use [7zip](https://www.7-zip.org/download.html) to decompress the data (you will have to modify the cell below).\n",
        "\n",
        "**What to submit:**\n",
        "* Convert the notebook to a PDF file by compiling it, and submit the PDF file.\n",
        "* Make sure all cells are executed so all your code and its results are included.\n",
        "* Double-check that the PDF displays correctly before you submit it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads and extracts the data. These commands download and extract the\n",
        "# email datasets from a specified source.\n",
        "# They work in a Linux environment, which is the base of Google Colab. ASDF"
      ],
      "metadata": {
        "id": "66WtvUpa5K3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa37fBwRF-xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810d3e92-d3c7-4fa0-fda7-dc91802e517f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-27 16:10:33--  https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1677144 (1.6M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_easy_ham.tar.bz2.3’\n",
            "\n",
            "\r          20021010_   0%[                    ]       0  --.-KB/s               \r20021010_easy_ham.t 100%[===================>]   1.60M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-27 16:10:33 (144 MB/s) - ‘20021010_easy_ham.tar.bz2.3’ saved [1677144/1677144]\n",
            "\n",
            "--2023-11-27 16:10:33--  https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1021126 (997K) [application/x-bzip2]\n",
            "Saving to: ‘20021010_hard_ham.tar.bz2.3’\n",
            "\n",
            "20021010_hard_ham.t 100%[===================>] 997.19K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-27 16:10:33 (90.3 MB/s) - ‘20021010_hard_ham.tar.bz2.3’ saved [1021126/1021126]\n",
            "\n",
            "--2023-11-27 16:10:33--  https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1192582 (1.1M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_spam.tar.bz2.3’\n",
            "\n",
            "20021010_spam.tar.b 100%[===================>]   1.14M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-11-27 16:10:33 (107 MB/s) - ‘20021010_spam.tar.bz2.3’ saved [1192582/1192582]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading datasets\n",
        "\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
        "\n",
        "# Extracting the datasets\n",
        "\n",
        "!tar -xjf 20021010_easy_ham.tar.bz2\n",
        "!tar -xjf 20021010_hard_ham.tar.bz2\n",
        "!tar -xjf 20021010_spam.tar.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH1XTepLjZ3"
      },
      "source": [
        "The data is now in the following three folders: `easy_ham`, `hard_ham`, and `spam`. You can confirm this via the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A53Gw00fBLG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd686c3-8166-48d5-cb31-d1e142050fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16M\n",
            "drwxr-xr-x 1 root root 4.0K Nov 27 16:10 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov 27 15:16 ..\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.1\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.2\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2.3\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.1\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.2\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2.3\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.1\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.2\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2.3\n",
            "drwxr-xr-x 4 root root 4.0K Nov 21 14:21 .config\n",
            "drwx--x--x 2  500  500 168K Oct 10  2002 easy_ham\n",
            "drwx--x--x 2 1000 1000  20K Dec 16  2004 hard_ham\n",
            "drwxr-xr-x 1 root root 4.0K Nov 21 14:24 sample_data\n",
            "drwxr-xr-x 2  500  500  36K Oct 10  2002 spam\n"
          ]
        }
      ],
      "source": [
        "# Listing the downloaded files\n",
        "\n",
        "!ls -lah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlWPVnSNzT7"
      },
      "source": [
        "### 1. Preprocessing:\n",
        "\n",
        "In the pre-processing section, the code outputs the number of emails in each of the datasets:\n",
        "1. Easy ham\n",
        "2. Spam\n",
        "3. Hard ham\n",
        "\n",
        "These print statements provide a clear understanding of the volume of data that the model will be processing.\n",
        "By knowing the number of emails in each category, one can gauge the dataset's diversity and size, which are vital factors in determining the robustness and reliability of the training process.\n",
        "This information ca be good to know to understand how it influences the model's learning and eventual performance.\n",
        "\n",
        "Note: The email files contain a lot of extra information, besides the actual message. This will momentaily be ignored meaning that the code will be ran on the entire text.\n",
        "\n",
        "We don’t want to train and test on the same data.An example that explains the reason for this is the following:\n",
        "\n",
        "Training and testing on the same data is akin to memorizing answers for an upcoming test. You will only be good at answering these specific questions but not good at other questions in the same topic.\n",
        "\n",
        "In other words, our approach gives us a more accurate measure of how well the model generalizes to new data, which is the ultimate goal of most machine learning models.\n",
        "\n",
        " 1. Split the spam and ham datasets into a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`). Use `easy_ham` for quesions 1 and 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2sllUWXKblD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059f6da8-fe88-4cb4-bac0-7a321283f298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Easy ham: 2551\n",
            "Spam: 501\n",
            "Hard ham: 250\n"
          ]
        }
      ],
      "source": [
        "# Used for accessing the file system.\n",
        "import os\n",
        "\n",
        "# For splitting data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to read files from a directory into a list\n",
        "\n",
        "def read_email_files(directory):\n",
        "  # Reads email files from a given directory and returns them in a list\n",
        "\n",
        "    files = os.listdir(directory) # Lists all files in the directory\n",
        "    emails = []\n",
        "    for file in files:\n",
        "        with open(os.path.join(directory, file), 'r', encoding='latin1') as email_file:\n",
        "            emails.append(email_file.read()) # Reads each file and appends its content\n",
        "    return emails\n",
        "\n",
        "# Reading and storing emails from directories\n",
        "easy_ham = read_email_files('/content/easy_ham')\n",
        "spam = read_email_files('/content/spam')\n",
        "hard_ham = read_email_files('/content/hard_ham')\n",
        "\n",
        "# Splitting data into training and test sets\n",
        "hamtrain, hamtest = train_test_split(easy_ham, test_size=0.2, random_state=42)\n",
        "spamtrain, spamtest = train_test_split(spam, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prints amount of email files in respective folders\n",
        "print('Easy ham:', + len(easy_ham))\n",
        "print('Spam:', + len(spam))\n",
        "print('Hard ham:', + len(hard_ham))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell's output shows the amount of email files in the three respective folders."
      ],
      "metadata": {
        "id": "7zfLRCmB1zvR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnbrbI0_OKCF"
      },
      "source": [
        "### 2. Write a Python program:\n",
        "\n",
        "As we move to the model evaluation outputs, the evaluate_model function comes into play, providing a detailed look at the model's performance through two key metrics: True Positives and False Negatives.\n",
        "\n",
        "The Python program should be written in the steps below:\n",
        "\n",
        "1.\tUses the four datasets from Question 1 (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`).\n",
        "2.\tTrains a Naïve Bayes classifier (use the [scikit-learn library](https://scikit-learn.org/stable/)) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)) to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifiers available in *scikit-learn* ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Here, you will test two of these classifiers that are well suited for this problem:\n",
        "- Multinomial Naive Bayes\n",
        "- Bernoulli Naive Bayes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJERHSCcGNaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6703412-bb18-47d2-e66b-5aa3bf403df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes:\n",
            "Amount of True Positives (TP): 72\n",
            "Amount of False Negatives (FN): 29\n",
            "True Positive Rate: 71.0%\n",
            "False Negative Rate: 28.999999999999996%\n",
            "\n",
            "Bernoulli Naive Bayes:\n",
            "Amount of True Positives (TP): 38\n",
            "Amount of False Negatives (FN): 63\n",
            "True Positive Rate: 38.0%\n",
            "False Negative Rate: 62.0%\n"
          ]
        }
      ],
      "source": [
        "# Needed modules to perform task 2:\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Converts text to numerical vector\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB # Naive Bayes models\n",
        "from sklearn.metrics import confusion_matrix # For model evaluation\n",
        "\n",
        "\n",
        "train_emails = hamtrain + spamtrain # Combining training datasets\n",
        "test_emails = hamtest + spamtest # Combining test datasets\n",
        "\n",
        "\n",
        "train_labels = [0]*len(hamtrain) + [1]*len(spamtrain) # Creating labels for training\n",
        "test_labels = [0]*len(hamtest) + [1]*len(spamtest) # Creating labels for testing\n",
        "\n",
        "# Convert emails to a matrix of token counts\n",
        "\n",
        "vectorizer = CountVectorizer() # Initializing the vectorizer\n",
        "X_train = vectorizer.fit_transform(train_emails) # Vectorizing training data\n",
        "X_test = vectorizer.transform(test_emails) # Vectorizing test data\n",
        "\n",
        "# Training Multinomial Naive Bayes model\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, train_labels)\n",
        "\n",
        "# Training Bernoulli Naive Bayes model\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, train_labels)\n",
        "\n",
        "# Making predictions with both models\n",
        "mnb_pred = mnb.predict(X_test)\n",
        "bnb_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "\n",
        "def evaluate_model(predictions, labels):\n",
        "    # Evaluates the model by calculating True Positives and False Negatives\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
        "    print(\"Amount of True Positives (TP):\", tp)\n",
        "    print(\"Amount of False Negatives (FN):\", fn)\n",
        "    print(\"True Positive Rate:\", str(round(tp / (tp + fn), 2) * 100) + \"%\")\n",
        "    print(\"False Negative Rate:\", str(round(fn / (tp + fn), 2) * 100) + \"%\")\n",
        "\n",
        "# Evaluating both models\n",
        "\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "evaluate_model(mnb_pred, test_labels)\n",
        "print(\"\\nBernoulli Naive Bayes:\")\n",
        "evaluate_model(bnb_pred, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of True Positives represents the emails correctly identified as spam by the model, a direct measure of the model's accuracy and effectiveness in recognizing spam emails.\n",
        "\n",
        "On the other hand, False Negatives denote the spam emails that the model failed to identify, slipping through as false 'ham' or non-spam emails. This metric is equally important as it highlights the model's shortcomings and areas where it might be improved.\n",
        "\n",
        "These metrics demonstrates the model's performance, highlighting both its strengths in identifying spam and its weaknesses where improvements might be necessary."
      ],
      "metadata": {
        "id": "b8u6dJ0G4V2J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFS3uFFUcS7"
      },
      "source": [
        "### 3. Run on hard ham:\n",
        "Run the two models from Question 2 on `spam` versus `hard-ham`, and compare to the `easy-ham` results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gool_zb8Qzzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6cab0b-e2bc-4061-9ee7-8707e32ccfd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes:\n",
            "True Positives (TP): 101\n",
            "False Negatives (FN): 0\n",
            "True Positive Rate: 100.0%\n",
            "False Negative Rate: 0.0%\n",
            "\n",
            "Bernoulli Naive Bayes:\n",
            "True Positives (TP): 101\n",
            "False Negatives (FN): 0\n",
            "True Positive Rate: 100.0%\n",
            "False Negative Rate: 0.0%\n"
          ]
        }
      ],
      "source": [
        "train_emails_hard_ham = hard_ham + spam # Combining hard ham and spam for training\n",
        "train_labels_hard_ham = [0] * len(hard_ham) + [1] * len(spam)\n",
        "\n",
        "# Vectorize the training data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_hard_ham = vectorizer.fit_transform(train_emails_hard_ham) # Vectorizing hard ham data\n",
        "\n",
        "# Trains the Multinomial and Bernoulli Naive Bayes models on hard ham data\n",
        "mnb_hard_ham = MultinomialNB().fit(X_train_hard_ham, train_labels_hard_ham)\n",
        "bnb_hard_ham = BernoulliNB().fit(X_train_hard_ham, train_labels_hard_ham)\n",
        "\n",
        "X_test = vectorizer.transform(hamtest + spamtest)  # Transforming test data\n",
        "\n",
        "# Labels for the test set\n",
        "test_labels = [0] * len(hamtest) + [1] * len(spamtest)\n",
        "\n",
        "# Predicting using models trained on hard ham\n",
        "mnb_pred_hard_ham = mnb_hard_ham.predict(X_test)\n",
        "bnb_pred_hard_ham = bnb_hard_ham.predict(X_test)\n",
        "\n",
        "# Function to evaluate the model\n",
        "\n",
        "def evaluate_model(predictions, actual):\n",
        "    tn, fp, fn, tp = confusion_matrix(actual, predictions).ravel()\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\"True Positive Rate:\", str(round(tp / (tp + fn), 2) * 100) + \"%\")\n",
        "    print(\"False Negative Rate:\", str(round(fn / (tp + fn), 2) * 100) + \"%\")\n",
        "\n",
        "# Evaluating models trained on hard ham\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "evaluate_model(mnb_pred_hard_ham, test_labels)\n",
        "print(\"\\nBernoulli Naive Bayes:\")\n",
        "evaluate_model(bnb_pred_hard_ham, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the models, which were initially trained on the hard ham datasets, are put to the test against the test set. This step is essential to gauge the models' adaptability and effectiveness in dealing with more challenging and nuanced scenarios. Hard ham emails are typically more difficult to distinguish from spam, thus presenting a more rigorous test for the model.\n",
        "\n",
        "Evaluating the model's performance on this dataset provides insights into how well the model can adapt and perform when faced with complex, real-world data that doesn't always present clear-cut categorizations. This evaluation is important for  understanding the robustness and real-world applicability of the models in question."
      ],
      "metadata": {
        "id": "cytOqEUoURxb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}