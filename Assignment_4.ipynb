{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerimeterMaison/Assignment-4/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sTsDfIVKsmL"
      },
      "source": [
        "\n",
        "OBS: CTRL+F ASDF. ALLT SOM HAR ASDF EFTER SIG MÅSTE KORRIGERAS!\n",
        "\n",
        "/MoMoney\n",
        "\n",
        "# DAT565 Introduction to Data Science and AI\n",
        "## 2023-2024, LP2\n",
        "## Assignment 4: Spam classification using Naïve Bayes\n",
        "This assignment has three obligatory questions. Questions 4-5 are optional and will not be graded.\n",
        "\n",
        "\n",
        "The exercise takes place in this notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical.\n",
        "\n",
        "*Tips:*\n",
        "* You can execute certain Linux shell commands by prefixing the command with a `!`.\n",
        "* You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results, the second you can use to write code snippets that execute the tasks required.  \n",
        "\n",
        "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets.\n",
        "\n",
        "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location:\n",
        "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages.\n",
        "-\thard-ham: non-spam messages more difficult to differentiate\n",
        "-\tspam: spam messages\n",
        "\n",
        "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.**\n",
        "\n",
        "If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer. Note that if you are using Windows, you can instead use [7zip](https://www.7-zip.org/download.html) to decompress the data (you will have to modify the cell below).\n",
        "\n",
        "**What to submit:**\n",
        "* Convert the notebook to a PDF file by compiling it, and submit the PDF file.\n",
        "* Make sure all cells are executed so all your code and its results are included.\n",
        "* Double-check that the PDF displays correctly before you submit it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wa37fBwRF-xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dabf1a6-4807-44d9-fcdc-542b9fd1ebd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-25 12:18:52--  https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1677144 (1.6M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_easy_ham.tar.bz2’\n",
            "\n",
            "20021010_easy_ham.t 100%[===================>]   1.60M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-11-25 12:18:53 (20.2 MB/s) - ‘20021010_easy_ham.tar.bz2’ saved [1677144/1677144]\n",
            "\n",
            "--2023-11-25 12:18:53--  https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1021126 (997K) [application/x-bzip2]\n",
            "Saving to: ‘20021010_hard_ham.tar.bz2’\n",
            "\n",
            "20021010_hard_ham.t 100%[===================>] 997.19K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-11-25 12:18:53 (14.9 MB/s) - ‘20021010_hard_ham.tar.bz2’ saved [1021126/1021126]\n",
            "\n",
            "--2023-11-25 12:18:53--  https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1192582 (1.1M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_spam.tar.bz2’\n",
            "\n",
            "20021010_spam.tar.b 100%[===================>]   1.14M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-11-25 12:18:54 (15.9 MB/s) - ‘20021010_spam.tar.bz2’ saved [1192582/1192582]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloads and extracts the data. \"wget\" and \"tar\" are usually used in Linux\n",
        "# but can also be used in Google Colab notebook since Google Colab runs on\n",
        "# Linux.\n",
        "\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
        "!tar -xjf 20021010_easy_ham.tar.bz2\n",
        "!tar -xjf 20021010_hard_ham.tar.bz2\n",
        "!tar -xjf 20021010_spam.tar.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH1XTepLjZ3"
      },
      "source": [
        "The data is now in the following three folders: `easy_ham`, `hard_ham`, and `spam`. You can confirm this via the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A53Gw00fBLG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a595e39-546d-4e5b-c422-59b0fcc49158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.0M\n",
            "drwxr-xr-x 1 root root 4.0K Nov 24 20:55 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov 24 20:53 ..\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2\n",
            "drwxr-xr-x 4 root root 4.0K Nov 21 14:21 .config\n",
            "drwx--x--x 2  500  500 168K Oct 10  2002 easy_ham\n",
            "drwx--x--x 2 1000 1000  20K Dec 16  2004 hard_ham\n",
            "drwxr-xr-x 1 root root 4.0K Nov 21 14:24 sample_data\n",
            "drwxr-xr-x 2  500  500  36K Oct 10  2002 spam\n"
          ]
        }
      ],
      "source": [
        "!ls -lah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlWPVnSNzT7"
      },
      "source": [
        "### 1. Preprocessing:\n",
        "Note that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text (in the optional part further down, you can experiment with filtering out the headers and footers).\n",
        "1.\tWe don’t want to train and test on the same data (it might help to reflect on **why** ,if you don't recall). Split the spam and ham datasets into a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`). Use `easy_ham` for quesions 1 and 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2sllUWXKblD"
      },
      "outputs": [],
      "source": [
        "# Write your pre-processing code here\n",
        "\n",
        "import os\n",
        "# import re #Works like a magnifying glass meaning that it is used to find\n",
        "            #specific/unique words, no need for it right now! ASDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to read files from a directory into a list\n",
        "def read_email_files(directory):\n",
        "    files = os.listdir(directory)\n",
        "    emails = []\n",
        "    for file in files:\n",
        "        with open(os.path.join(directory, file), 'r', encoding='latin1') as email_file:\n",
        "            emails.append(email_file.read())\n",
        "    return emails\n",
        "\n",
        "# Assuming the directories are named 'easy_ham' and 'spam' in your current working directory\n",
        "easy_ham = read_email_files('/content/easy_ham')\n",
        "spam = read_email_files('/content/spam')\n",
        "\n",
        "# Split each dataset into training and test sets\n",
        "hamtrain, hamtest = train_test_split(easy_ham, test_size=0.2, random_state=42)\n",
        "spamtrain, spamtest = train_test_split(spam, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now hamtrain, hamtest, spamtrain, and spamtest contain your actual email data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnbrbI0_OKCF"
      },
      "source": [
        "### 2. Write a Python program that:\n",
        "1.\tUses the four datasets from Question 1 (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`).\n",
        "2.\tTrains a Naïve Bayes classifier (use the [scikit-learn library](https://scikit-learn.org/stable/)) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)) to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifiers available in *scikit-learn* ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Here, you will test two of these classifiers that are well suited for this problem:\n",
        "- Multinomial Naive Bayes\n",
        "- Bernoulli Naive Bayes.\n",
        "\n",
        "Please inspect the documentation to ensure input to the classifiers is appropriate before you start coding. You may have to modify your input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MJERHSCcGNaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991799dc-f363-4290-c558-aadc44236c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes:\n",
            "True Positives (TP): 72\n",
            "False Negatives (FN): 29\n",
            "\n",
            "Bernoulli Naive Bayes:\n",
            "True Positives (TP): 38\n",
            "False Negatives (FN): 63\n"
          ]
        }
      ],
      "source": [
        "# write your code here\n",
        "\n",
        "#What does the code do?\n",
        "\n",
        "# The code reads email data from directories, splits it into training and\n",
        "# testing datasets, converts the email texts into numerical vectors, and then\n",
        "# uses these vectors to train and evaluate two types of Naïve Bayes classifiers,\n",
        "# Multinomial and Bernoulli - by calculating their\n",
        "# True Positives and False Negatives.\n",
        "\n",
        "# The reason the email texts are converted to numerical value is because\n",
        "#  ASDF\n",
        "\n",
        "# We already have our email data split into training and test sets for both\n",
        "# 'ham' (non-spam) and 'spam' emails from Task 1. Next step is to convert\n",
        "# these text emails into a format that our machine learning model\n",
        "# can understand.\n",
        "# This is done using CountVectorizer from scikit-learn, which converts text\n",
        "#  data into a matrix of token counts.\n",
        "\n",
        "\n",
        "# Needed modules to perform task 2:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Combines ham and spam datasets\n",
        "train_emails = hamtrain + spamtrain\n",
        "test_emails = hamtest + spamtest\n",
        "\n",
        "# Create labels\n",
        "train_labels = [0]*len(hamtrain) + [1]*len(spamtrain)\n",
        "test_labels = [0]*len(hamtest) + [1]*len(spamtest)\n",
        "\n",
        "# Convert emails to a matrix of token counts\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_emails)\n",
        "X_test = vectorizer.transform(test_emails)\n",
        "\n",
        "# Train the Multinomial Naive Bayes model\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, train_labels)\n",
        "\n",
        "# Train the Bernoulli Naive Bayes model\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, train_labels)\n",
        "\n",
        "# Predictions\n",
        "mnb_pred = mnb.predict(X_test)\n",
        "bnb_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(predictions, labels):\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "evaluate_model(mnb_pred, test_labels)\n",
        "\n",
        "print(\"\\nBernoulli Naive Bayes:\")\n",
        "evaluate_model(bnb_pred, test_labels)\n",
        "\n",
        "# The explanation for the output produced from this code cell\n",
        "# (outputs being the TP and  FN) is the following: ASDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFS3uFFUcS7"
      },
      "source": [
        "### 3. Run on hard ham:\n",
        "Run the two models from Question 2 on `spam` versus `hard-ham`, and compare to the `easy-ham` results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gool_zb8Qzzy"
      },
      "outputs": [],
      "source": [
        "# code to report results here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkfQWBB4UhYd"
      },
      "source": [
        "### 4.\tOPTIONAL - NOT MARKED:\n",
        "To avoid classification based on common and uninformative words, it is common practice to filter these out.\n",
        "\n",
        "**a.** Think about why this may be useful. Show a few examples of too common and too uncommon words.\n",
        "\n",
        "**b.** Use the parameters in *scikit-learn*’s `CountVectorizer` to filter out these words. Update the program from Question 2 and run it on `easy-ham` vs `spam` and `hard-ham` vs `spam`. Report your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt7ELzEqUfas"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcyVfOZFU4F_"
      },
      "source": [
        "### 5. OPTIONAL - NOT MARKED: Further improving performance\n",
        "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions:\n",
        "- Does the result improve from those obtained in Questions 3 and 4?\n",
        "- What do you expect would happen if your training set consisted mostly of spam messages, while your test set consisted mostly of ham messages, or vice versa?\n",
        "- Look at the `fit_prior` parameter. What does this parameter mean? Discuss in what settings it can be helpful (you can also test your hypothesis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkIB6h9k4r07"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}